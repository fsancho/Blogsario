     <meta charset="utf-8" emacsmode="-*- markdown -*-" lang="es">
	 <!-- <link rel="stylesheet" href="markdeep-docs.css?">-->
	 <link rel="stylesheet" href="newsmag.css?">

                          **<a href="atdbi.md.html" style="text-decoration: none; color: inherit;">ATDBI</a>**
    Introducción
	Sobre la Toma de Decisiones Bajo Incertidumbre


Muchos problemas importantes implican la toma de decisiones en condiciones de incertidumbre, como la evitación de colisiones aéreas, la gestión de incendios forestales y la respuesta ante catástrofes. A la hora de diseñar sistemas automatizados de toma de decisiones o sistemas de apoyo a la toma de decisiones, es importante tener en cuenta diversas fuentes de incertidumbre y equilibrar cuidadosamente los múltiples objetivos. En este capítulo se analizan estos retos desde una perspectiva computacional, con el objetivo de proporcionar la teoría que sustenta los modelos de toma de decisiones y los enfoques computacionales. En este capítulo se presenta el problema de la toma de decisiones en condiciones de incertidumbre, se ofrecen algunos ejemplos de aplicaciones y se esboza el espacio de los enfoques computacionales. A continuación, se resume el modo en que diversas disciplinas han contribuido a nuestra comprensión de la toma de decisiones inteligente y se destacan las áreas de posible impacto social. Concluimos con un resumen del resto del libro.

# 1.1 Toma de decisiones

![](./img/Fig1-1.png align=right width=40%)Un **agente** es una entidad que actúa basándose en observaciones de su entorno. Los agentes pueden ser entidades físicas, como los seres humanos o los robots, o pueden ser entidades no físicas, como los sistemas de apoyo a la toma de decisiones que se implementan completamente en software. Como se muestra en la figura 1.1, la interacción entre el agente y el entorno sigue un *ciclo de observación-acción*. El agente, en el momento $t$, recibe una observación del entorno, denotada como $o_t$. Las observaciones pueden realizarse, por ejemplo, a través de un proceso sensorial biológico, como en el caso de los seres humanos, o mediante un sistema de sensores, como el radar en un sistema de control del tráfico aéreo. Las observaciones son a menudo incompletas o ruidosas; los humanos pueden no ver un avión que se aproxima o un sistema de radar puede pasar por alto una detección debido a la interferencia electromagnética. El agente elige entonces una acción, $a_t$ a través de un proceso de *toma de decisiones*.

Esta acción, como la emisión de una alerta, puede tener un efecto no determinista en el entorno.

Nos centramos en los agentes que interactúan de forma inteligente para alcanzar sus objetivos a lo largo del tiempo (nos enfocaremos en problemas con tiempo discreto, no continuo, que son comúnmente estudiados en *Teoría de Control*). Dada la secuencia pasada de observaciones, $o_1,\dots , o_t$, y el conocimiento del entorno, el agente debe elegir una acción en la que consiga sus objetivos de la mejor manera posible en presencia de varias fuentes de incertidumbre , entre ellas podemos destacar fundamentalmente las cuatro siguientes:

- *Incertidumbre en los resultados*, donde los efectos de nuestras acciones son inciertos.
- *Incertidumbre del modelo*, en la que nuestro modelo del problema es incierto.
- *Incertidumbre de estado*, cuando el estado real del entorno es incierto.
- *Incertidumbre de interacción*, cuando el comportamiento de los demás agentes que interactúan en el entorno es incierto.

En este curso nos organizaremos en torno a estas cuatro fuentes de incertidumbre. La toma de decisiones en presencia de la incertidumbre es fundamental en el campo de la Inteligencia Artificial, así como en muchos otros campos. Los algoritmos que mostraremos para tomar decisiones tendrán como requisitos ser robustos a la incertidumbre.

# 1.2 Aplicaciones

El marco de toma de decisiones presentado en el apartado anterior puede aplicarse a una gran variedad de ámbitos. A continuación se analizan algunos ejemplos conceptuales con aplicaciones en el mundo real. En el Apéndice F se describen algunos problemas paradigmáticos que se utilizarán a lo largo de este texto como demostración y zona de pruebas de los algoritmos que comentamos.


## 1.2.1 Evitar la colisión de aviones

![](./img/colision.jpg align=right width=30%)Para ayudar a prevenir las colisiones en el aire entre aeronaves, queremos diseñar un sistema que pueda alertar a los pilotos de posibles amenazas e indicarles cómo maniobrar para evitarlas. El sistema se comunica con los transpondedores de otras aeronaves para identificar sus posiciones con cierto grado de precisión. Decidir qué orientación proporcionar a los pilotos es un reto. No se sabe con certeza con qué rapidez responderán los pilotos y con qué agresividad cumplirán las directrices. Además, existe incertidumbre sobre el comportamiento de otras aeronaves. Queremos que nuestro sistema alerte con suficiente antelación para que los pilotos tengan tiempo de maniobrar sus aviones para evitar la colisión, pero no queremos que nuestro sistema emita alertas con demasiada antelación, lo que daría lugar a muchas maniobras innecesarias. Dado que este sistema se utilizará continuamente en todo el mundo, necesitamos que el sistema ofrezca un nivel de seguridad excepcional.

## 1.2.2 Conducción autónoma

![](./img/ConduccionAutonoma.jpg align=left width=30%)Queremos construir un vehículo autónomo que pueda conducir de forma segura en entornos urbanos. El vehículo debe basarse en un conjunto de sensores para percibir su entorno con el fin de tomar decisiones seguras. Un tipo de sensor es el lidar, que consiste en medir los reflejos láser del entorno para determinar las distancias a los obstáculos. Otro tipo de sensor es la cámara, que, mediante algoritmos de visión por ordenador, puede detectar peatones y otros vehículos. Ambos tipos de sensores son imperfectos y susceptibles de sufrir ruidos y oclusiones. Por ejemplo, un camión aparcado puede ocluir a un peatón que intente cruzar por un paso de peatones. Nuestro sistema debe predecir las intenciones y las trayectorias futuras de otros vehículos, peatones y otros usuarios de la carretera a partir de sus comportamientos observables para poder navegar con seguridad hasta nuestro destino.

## 1.2.3 Detección del cáncer de mama

![](./img/CancerMama.jpg align=right width=30%)El cáncer de mama es el más frecuente en las mujeres de todo el mundo. La detección precoz del cáncer de mama puede ayudar a salvar vidas, siendo la mamografía la herramienta de cribado más eficaz disponible. Sin embargo, la mamografía conlleva riesgos potenciales, como los falsos positivos, que pueden dar lugar a un seguimiento diagnóstico innecesario e invasivo. Las investigaciones realizadas a lo largo de los años han dado lugar a diversos programas de cribado basados en la edad para equilibrar los beneficios y los riesgos de las pruebas. El desarrollo de un sistema que pueda hacer recomendaciones basadas en las características de riesgo personales y en el historial de cribado tiene el potencial de producir mejores resultados sanitarios. El éxito de un sistema de este tipo puede compararse con los programas de cribado a nivel poblacional en términos de años de vida totales ajustados a la calidad esperada, el número de mamografías, la prevalencia de falsos positivos y el riesgo de cáncer invasivo no detectado.

## 1.2.4 Consumo financiero y asignación de carteras

![](./img/carteras.jpg align=left width=30%)Supongamos que queremos construir un sistema que recomiende qué parte de la riqueza de un individuo debe consumirse ese año y qué parte debe invertirse. La cartera de inversión puede incluir acciones y bonos con diferentes niveles de riesgo y rendimiento esperado. La evolución de la riqueza es estocástica debido a la incertidumbre de los ingresos, tanto del trabajo como de la inversión, y suele aumentar hasta que el inversor se acerca a la jubilación, y luego disminuye constantemente. El disfrute que supone el consumo de una unidad de riqueza en un año suele disminuir con la cantidad consumida, lo que hace que se desee suavizar el consumo a lo largo de la vida del individuo.

## 1.2.5 Vigilancia distribuida de los incendios forestales

![](./img/Deerfire.jpg align=right width=30%)El conocimiento de la situación es un reto importante en la lucha contra los incendios forestales. El estado de un incendio evoluciona con el tiempo, influenciado por factores como el viento y la distribución del combustible en el entorno. Muchos incendios forestales abarcan grandes regiones geográficas. Un concepto para vigilar un incendio forestal es utilizar un equipo de drones equipados con sensores que vuelen por encima de él. El alcance de detección de los drones individuales es limitado, pero la información del equipo puede fusionarse para proporcionar una instantánea unificada de la situación que permita tomar decisiones sobre la asignación de recursos. Nos gustaría que los miembros del equipo determinaran de forma autónoma cómo colaborar entre sí para proporcionar la mejor cobertura del incendio. Una vigilancia eficaz requiere decidir cómo maniobrar para cubrir las zonas en las que es probable que la nueva información de los sensores sea útil; pasar tiempo en zonas en las que estamos seguros de si el incendio está ardiendo o no sería un desperdicio. La identificación de áreas importantes para explorar requiere razonar sobre la evolución estocástica del fuego, dado sólo un conocimiento imperfecto de su estado actual.

## 1.2.6 Exploración científica de Marte

![](./img/ExploracionMarte.jpg align=left width=30%)Los vehículos de exploración han realizado importantes descubrimientos en Marte y han aumentado nuestra comprensión del mismo. Sin embargo, un importante cuello de botella en la exploración científica ha sido el enlace de comunicación entre el rover y el equipo de operaciones en la Tierra. La información de los sensores puede tardar hasta media hora en enviarse desde Marte a la Tierra y otro tanto las órdenes en enviarse desde la Tierra a Marte. Además, es necesario planificar con antelación la orientación de los rovers, ya que las ventanas de carga y descarga con Marte son limitadas debido a las posiciones de los orbitadores que sirven de relés de información entre los planetas. Investigaciones recientes han sugerido que la eficiencia de las misiones de exploración científica puede quintuplicarse mediante la introducción de mayores niveles de autonomía. Los operadores humanos seguirían proporcionando una guía de alto nivel sobre los objetivos de la misión, pero el rover tendría la flexibilidad de seleccionar sus propios objetivos científicos utilizando la información más actualizada. Además, sería deseable que los rovers respondieran adecuadamente a diversos peligros y fallos del sistema sin intervención humana.

# 1.3 Métodos

Existen muchos métodos para diseñar agentes de decisión. Dependiendo de la aplicación, algunos pueden ser más apropiados que otros. Difieren en las responsabilidades del diseñador y en las tareas que se dejan a la automatización. En esta sección se presenta brevemente una colección de estos métodos. El libro se centrará principalmente en la planificación y el aprendizaje por refuerzo, pero algunas de las técnicas incluirán elementos de aprendizaje supervisado y optimización.

## 1.3.1 Programación explícita

![](./img/programacion.jpeg align=right width=30%)El método más directo para diseñar un agente con capacidad de decisión es anticipar todos los escenarios en los que el agente podría encontrarse y programar explícitamente lo que el agente debería hacer en respuesta a cada uno de ellos. El enfoque de programación explícita puede funcionar bien para problemas sencillos, pero supone una gran carga para el diseñador a la hora de proporcionar una estrategia completa. Se han propuesto varios lenguajes y marcos de programación de agentes para facilitar la programación de los mismos.

## 1.3.2 Aprendizaje supervisado

![](./img/SupervisedLearning.png align=left width=30%)En algunos problemas, puede ser más fácil mostrar a un agente lo que debe hacer que escribir un programa para que lo siga. El diseñador proporciona un conjunto de ejemplos de entrenamiento, y un algoritmo de aprendizaje automatizado debe generalizar a partir de estos ejemplos. Este enfoque se conoce como aprendizaje supervisado y se ha aplicado ampliamente a los problemas de clasificación. Esta técnica se denomina a veces *clonación del comportamiento* cuando se aplica al aprendizaje de mapeos de observaciones a acciones. La clonación del comportamiento funciona bien cuando un diseñador experto conoce realmente la elección de la mejor acción para una colección representativa de situaciones. Aunque existe una gran variedad de algoritmos de aprendizaje, por lo general no pueden funcionar mejor que los diseñadores humanos en situaciones nuevas.

## 1.3.3 Optimización

![](./img/Optimization.png align=right width=30%)Otro enfoque consiste en que el diseñador especifique el espacio de posibles estrategias de decisión y una medida de rendimiento que debe maximizarse. La evaluación del rendimiento de una estrategia de decisión suele implicar la ejecución de un lote de simulaciones. El algoritmo de optimización realiza entonces una búsqueda de la estrategia óptima en este espacio. Si el espacio es relativamente pequeño y la medida de rendimiento no tiene muchos óptimos locales, entonces pueden ser apropiados varios métodos de búsqueda local o global. Aunque generalmente se asume el conocimiento de un modelo dinámico para realizar las simulaciones, no se utiliza para guiar la búsqueda, lo que puede ser importante para problemas complejos.

## 1.3.4 Planificación

![](./img/Planning.png align=left width=30%)La planificación es una forma de optimización que utiliza un modelo de la dinámica del problema para ayudar a guiar la búsqueda. Una gran parte de la literatura explora diversos problemas de planificación, gran parte de ella centrada en problemas deterministas. Para algunos problemas puede ser aceptable aproximar la dinámica con un modelo determinista. Asumir un modelo determinista nos permite utilizar métodos que pueden escalar más fácilmente a problemas de alta dimensión. Para otros problemas es fundamental tener en cuenta la incertidumbre futura. Este libro se centra por completo en los problemas en los que es importante tener en cuenta la incertidumbre.

## 1.3.5 Aprendizaje por refuerzo

![](./img/RL.png align=right width=30%)El aprendizaje por refuerzo relaja la suposición de la planificación de que se conoce un modelo de antemano. En su lugar, la estrategia de toma de decisiones se aprende mientras el agente interactúa con el entorno. El diseñador sólo tiene que proporcionar una medida de rendimiento; es el algoritmo de aprendizaje el que tiene que optimizar el comportamiento del agente. Una de las complejidades interesantes que surgen en el aprendizaje por refuerzo es que la elección de la acción no sólo afecta al éxito inmediato del agente en la consecución de sus objetivos, sino también a la capacidad del agente para aprender sobre el entorno e identificar las características del problema que puede explotar.

# 1.4 Historia

La teoría de la automatización del proceso de toma de decisiones tiene sus raíces en los sueños de los primeros filósofos, científicos, matemáticos y escritores. Los antiguos griegos empezaron a incorporar la automatización en mitos e historias ya en el año 800 a.C. La palabra *autómata* se utilizó por primera vez en la Ilíada de Homero, que contiene referencias a la noción de máquinas automáticas, incluidos los trípodes mecánicos utilizados para servir a los comensales. En el siglo XVII, los filósofos propusieron el uso de reglas lógicas para resolver automáticamente los desacuerdos. Sus ideas sentaron las bases del razonamiento mecanizado.

A partir de finales del siglo XVIII, los inventores empezaron a crear máquinas automáticas para realizar el trabajo. En particular, una serie de innovaciones en la industria textil condujeron al desarrollo del telar automático, que a su vez sentó las bases de los primeros robots de fábrica. A principios del siglo XIX, el uso de máquinas inteligentes para automatizar el trabajo comenzó a abrirse paso en las novelas de ciencia ficción. La palabra robot se originó en la obra del escritor checo Karel Čapek titulada *R.U.R.*, abreviatura de *Rossum's Universal Robots*, sobre máquinas que podían realizar trabajos que los humanos preferirían no hacer. La obra inspiró a otros escritores de ciencia ficción a incorporar robots en sus escritos. A mediados del siglo XX, el notable escritor y profesor Isaac Asimov expuso su visión de la robótica en su famosa serie *Robot*.

Uno de los principales retos de la aplicación práctica de la toma de decisiones automatizada es tener en cuenta la incertidumbre. Incluso a finales del siglo XX, George Dantzig, más famoso por el desarrollo del algoritmo simplex, declaró en 1991: "En retrospectiva, es interesante observar que el problema original que inició mi investigación sigue vigente, a saber, el problema de la planificación o programación dinámica en el tiempo, en particular la planificación dinámica bajo incertidumbre. Si este problema pudiera resolverse con éxito, podría contribuir (eventualmente, a través de una mejor planificación) al bienestar y la estabilidad del mundo".

Aunque la toma de decisiones bajo incertidumbre sigue siendo un área de investigación activa, en los últimos siglos los investigadores e ingenieros se han acercado a hacer posibles los conceptos planteados por estos primeros soñadores. Los algoritmos actuales de toma de decisiones se basan en una convergencia de conceptos desarrollados en múltiples disciplinas, como la economía, la psicología, la neurociencia, la informática, la ingeniería, las matemáticas y la investigación operativa. Esta sección destaca algunas de las principales contribuciones de estas disciplinas. La polinización cruzada entre disciplinas ha dado lugar a muchos avances recientes y probablemente seguirá apoyando el crecimiento en el futuro.

## 1.4.1 Economía

![](./img/Economia.jpg align=right width=30%)La economía requiere modelos de toma de decisiones humanas. Uno de los enfoques para construir estos modelos es la *teoría de la utilidad*, que se introdujo por primera vez a finales del siglo XVIII. La teoría de la utilidad proporciona un medio para modelar y comparar la conveniencia de varios resultados. En la *Teoría de la Legislación*, Jeremy Bentham resumió la no linealidad de la utilidad del dinero:

1. A cada porción de riqueza le corresponde una porción de felicidad.
2. De dos individuos con fortunas desiguales, el que tiene más riqueza tiene más felicidad.
3. El exceso de felicidad del más rico no será tan grande como el exceso de su riqueza.

Al combinar el concepto de utilidad con la noción de decisión racional, los economistas de mediados del siglo XX establecieron la base del principio de máxima utilidad esperada. Este principio es un concepto clave para la creación de agentes de decisión autónomos. La teoría de la utilidad también dio lugar al desarrollo de la *teoría de juegos*, que intenta comprender el comportamiento de múltiples agentes que actúan en presencia de otros para maximizar sus intereses.

## 1.4.2 Psicología

![](./img/Psicologia.jpg align=left width=30%)Los psicólogos también estudian la toma de decisiones humanas, normalmente desde la perspectiva del comportamiento humano. Mediante el estudio de las reacciones de los animales a los estímulos, los psicólogos han desarrollado teorías sobre el aprendizaje por ensayo y error desde el siglo XIX. Los investigadores observaron que los animales tienden a tomar decisiones en función de la satisfacción o el malestar que han experimentado en situaciones similares anteriores. El psicólogo ruso Ivan Pavlov combinó esta idea con el concepto de refuerzo tras observar los patrones de salivación de los perros cuando se les da de comer. El psicólogo descubrió que un patrón de comportamiento podía reforzarse o debilitarse mediante el refuerzo continuo de un estímulo concreto. A mediados del siglo XX, el matemático e informático Alan Turing expresó la posibilidad de que las máquinas aprendieran del mismo modo:

"La organización de una máquina en una máquina universal sería más impresionante si los arreglos de interferencia implicaran muy pocas entradas. El entrenamiento de un niño humano depende en gran medida de un sistema de premios y castigos, y esto sugiere que debería ser posible llevar a cabo la organización con sólo dos entradas de interferencia, una para el *placer* o *recompensa* (R) y la otra para el *dolor* o *castigo* (P)."

Los trabajos de los psicólogos sentaron las bases del campo del *aprendizaje por refuerzo*, una técnica fundamental para enseñar a los agentes a tomar decisiones en entornos inciertos.

## 1.4.3 Neurociencia

![](./img/Neurociencia.png align=right width=30%)Mientras que los psicólogos estudian el comportamiento humano tal y como se produce, los neurocientíficos se centran en los procesos biológicos utilizados para crear el comportamiento. A finales del siglo XIX, los científicos descubrieron que el cerebro está compuesto por una red interconectada de neuronas, responsable de su capacidad para percibir y razonar sobre el mundo. El pionero de la inteligencia artificial, Nils Nilsson, describe así la aplicación de estos hallazgos a la toma de decisiones: "Dado que el cerebro de un animal es el responsable de convertir la información sensorial en acción, es de esperar que se puedan encontrar varias buenas ideas en el trabajo de los neurofisiólogos y neuroanatomistas que estudian los cerebros y sus componentes fundamentales, las neuronas."

En la década de 1940, los investigadores propusieron por primera vez que las neuronas podían considerarse "unidades lógicas" individuales capaces de realizar operaciones computacionales cuando se unían en una red. Este trabajo sirvió de base para las redes neuronales, que se utilizan en el campo de la inteligencia artificial para realizar diversas tareas complejas.

## 1.4.4 La informática

![](./img/informatica.jpg align=left width=30%)A mediados del siglo XX, los informáticos empezaron a formular el problema de la toma de decisiones inteligente como un problema de manipulación simbólica a través de la lógica formal. El programa informático *Logic Theorist*, escrito a mediados del siglo XX para realizar razonamientos automatizados, utilizaba esta forma de pensar para demostrar teoremas matemáticos. Herbert Simon, uno de sus inventores, abordó la naturaleza simbólica del programa relacionándolo con la mente humana: "Hemos inventado un programa informático capaz de pensar de forma no numérica, y con ello hemos resuelto el venerable problema mente/cuerpo, explicando cómo un sistema compuesto de materia puede tener las propiedades de la mente."

Estos sistemas simbólicos dependían en gran medida de la experiencia humana. Un enfoque alternativo de la inteligencia, denominado *conexionismo*, se inspiró en parte en los avances de la neurociencia y se centra en el uso de redes neuronales artificiales como sustrato de la inteligencia. Con el conocimiento de que las redes neuronales pueden entrenarse para el reconocimiento de patrones, los conexionistas intentan aprender el comportamiento inteligente a partir de los datos o la experiencia, en lugar de los conocimientos codificados de los expertos. El paradigma conexionista sustentó el éxito de *AlphaGo*, el programa autónomo que venció a un profesional humano en el juego del Go, así como gran parte del desarrollo de los vehículos autónomos. Los algoritmos que combinan los paradigmas simbólico y conexionista siguen siendo un área de investigación activa en la actualidad.

## 1.4.5 Ingeniería

![](./img/Ingenieria.jpg align=right width=30%)El campo de la ingeniería se ha centrado en permitir que los sistemas físicos, como los robots, tomen decisiones inteligentes. El mundialmente conocido robotista Sebastian Thrun describe los componentes de estos sistemas de la siguiente manera: "Los sistemas robóticos tienen en común que están situados en el mundo físico, perciben su entorno a través de sensores y manipulan su entorno a través de cosas que se mueven."

Para diseñar estos sistemas, los ingenieros deben abordar la percepción, la planificación y la actuación. Los sistemas físicos perciben el mundo utilizando sus sensores para crear una representación de las características más destacadas de su entorno. El campo de la estimación del estado se ha centrado en utilizar las mediciones de los sensores para construir una creencia sobre el estado del mundo. La planificación requiere un razonamiento sobre las formas de ejecutar las tareas para las que han sido diseñados. El proceso de planificación ha sido posible gracias a los avances en la industria de los semiconductores durante muchas décadas. Una vez que se ha diseñado un plan, un agente autónomo debe actuar sobre él en el mundo real. Esta tarea requiere tanto hardware (en forma de actuadores) como algoritmos para controlar los actuadores y rechazar las perturbaciones. El campo de la *teoría de control* se ha centrado en la estabilización de los sistemas mecánicos mediante el control de la retroalimentación. Los sistemas de control automático se utilizan ampliamente en la industria, desde la regulación de la temperatura en un horno hasta la navegación de los sistemas aeroespaciales.

## 1.4.6 Matemáticas

![](./img/matematicas.jpg align=left width=30%)Un agente debe ser capaz de cuantificar su incertidumbre para tomar decisiones informadas en entornos inciertos. El campo de la toma de decisiones se basa en gran medida en la teoría de la probabilidad para esta tarea. En particular, la *estadística bayesiana* desempeña un papel importante en este texto. En 1763 se publicó póstumamente un artículo de Thomas Bayes que contenía lo que más tarde se conocería como la *regla de Bayes*. Su enfoque de la inferencia probabilística fue cayendo en desgracia hasta mediados del siglo XX, cuando los investigadores empezaron a encontrar la utilidad de los métodos bayesianos en una serie de escenarios. El matemático Bernard Koopman encontró un uso práctico para la teoría durante la Segunda Guerra Mundial:

"Toda operación de búsqueda está plagada de incertidumbres; sólo puede entenderse cuantitativamente en términos de [...] probabilidad."

Esta afirmación puede considerarse hoy en día una obviedad, pero parece que fueron necesarios los avances en la investigación operativa de la Segunda Guerra Mundial para hacer patentes sus implicaciones prácticas.

Los métodos basados en el muestreo (a veces denominados métodos de Montecarlo), desarrollados a principios del siglo XX para realizar cálculos a gran escala en el marco del *Proyecto Manhattan*, hicieron posibles algunas técnicas de inferencia que antes habrían sido intratables. Estos fundamentos sirven de base para las redes bayesianas, que aumentaron su popularidad más adelante en el siglo XX en el campo de la inteligencia artificial.

## 1.4.7 Investigación operativa

![](./img/IO.jpg align=right width=30%)La investigación operativa se ocupa de encontrar soluciones óptimas a los problemas de toma de decisiones, como la asignación de recursos, la inversión en activos y la programación del mantenimiento. A finales del siglo XIX, los investigadores comenzaron a explorar la aplicación del análisis matemático y científico a la producción de bienes y servicios. El campo se aceleró durante la Revolución Industrial, cuando las empresas empezaron a subdividir su gestión en departamentos responsables de distintos aspectos de las decisiones generales. Durante la Segunda Guerra Mundial, la optimización de las decisiones se aplicó a la asignación de recursos a un ejército. Una vez finalizada la guerra, las empresas empezaron a darse cuenta de que los mismos conceptos de investigación operativa utilizados anteriormente para tomar decisiones militares podían ayudarles a optimizar las decisiones empresariales. Esta constatación condujo al desarrollo de la ciencia de la gestión, descrita por el teórico de la organización Harold Koontz:

"La creencia permanente de este grupo es que, si la gestión, o la organización, o la planificación, o la toma de decisiones es un proceso lógico, puede expresarse en términos de símbolos y relaciones matemáticas. El enfoque central de esta escuela es el modelo, ya que es a través de estos dispositivos que el problema se expresa en sus relaciones básicas y en términos de metas u objetivos seleccionados."

Este deseo de poder modelar y comprender mejor las decisiones empresariales provocó el desarrollo de una serie de conceptos que se utilizan hoy en día, como la programación lineal, la programación dinámica y la teoría de colas.

# 1.5 Impacto en la sociedad

Los enfoques algorítmicos para la toma de decisiones han transformado la sociedad y probablemente seguirán haciéndolo en el futuro. En esta sección se destacan brevemente algunas formas en que los algoritmos de toma de decisiones pueden contribuir a la sociedad y se presentan los retos que quedan por delante cuando se intenta garantizar un beneficio amplio.

Los enfoques algorítmicos han contribuido a la sostenibilidad medioambiental. En el contexto de la gestión de la energía, por ejemplo, la optimización bayesiana se ha aplicado a los sistemas automatizados de gestión de la energía en el hogar. Los algoritmos del campo de los sistemas multiagentes se utilizan para predecir el funcionamiento de las redes inteligentes, diseñar mercados para el comercio de energía y predecir la adopción de la energía solar en los tejados. También se han desarrollado algoritmos para proteger la biodiversidad. Por ejemplo, las redes neuronales se utilizan para automatizar los censos de fauna, los enfoques teóricos de los juegos se utilizan para combatir la caza furtiva en los bosques y las técnicas de optimización se emplean para asignar recursos para la gestión del hábitat.

Los algoritmos de toma de decisiones han tenido éxito en el campo de la medicina durante décadas. Estos algoritmos se han utilizado para asignar residentes a hospitales y donantes de órganos a pacientes que los necesitan. Una de las primeras aplicaciones de las redes bayesianas, que trataremos en la primera parte de este libro, fue el diagnóstico de enfermedades. Desde entonces, las redes bayesianas se han utilizado ampliamente en medicina para el diagnóstico y el pronóstico de enfermedades. El campo del procesamiento de imágenes médicas se ha transformado gracias al aprendizaje profundo, y las ideas algorítmicas han desempeñado recientemente un papel importante en la comprensión de la propagación de enfermedades.

Los algoritmos han permitido comprender el crecimiento de las zonas urbanas y facilitar su diseño. Los algoritmos basados en datos se han utilizado ampliamente para mejorar las infraestructuras públicas. Por ejemplo, los procesos estocásticos se han utilizado para predecir fallos en las tuberías de agua, el aprendizaje profundo ha mejorado la gestión del tráfico, y los procesos de decisión de Markov y los métodos de Monte Carlo se han empleado para mejorar la respuesta a las emergencias. Las ideas de los sistemas multiagentes descentralizados han optimizado las rutas de viaje, y las técnicas de planificación de rutas se han utilizado para optimizar la entrega de mercancías. Los algoritmos de toma de decisiones se han utilizado para los coches autónomos y para mejorar la seguridad de los aviones.

Los algoritmos de optimización de decisiones pueden amplificar el impacto de sus usuarios, independientemente de su intención. Si el objetivo del usuario de estos algoritmos, por ejemplo, es difundir información errónea durante unas elecciones políticas, los procesos de optimización pueden ayudar a facilitarlo. Sin embargo, se pueden utilizar algoritmos similares para controlar y contrarrestar la difusión de información falsa. A veces, la aplicación de estos algoritmos de toma de decisiones puede conducir a consecuencias posteriores que sus usuarios no pretendían.

Aunque los algoritmos tienen el potencial de aportar importantes beneficios, también hay retos asociados a su aplicación en la sociedad. Los algoritmos basados en datos a menudo adolecen de sesgos y puntos ciegos inherentes debido a la forma en que se recogen los datos. A medida que los algoritmos pasan a formar parte de nuestras vidas, es importante entender cómo se puede reducir el riesgo de sesgo y cómo se pueden distribuir los beneficios del progreso algorítmico de manera equitativa y justa. Los algoritmos también pueden ser vulnerables a la manipulación por parte de los adversarios, y es fundamental que diseñemos algoritmos que sean resistentes a esos ataques. También es importante ampliar los marcos morales y legales para evitar consecuencias no deseadas y asignar responsabilidades.

# 1.6 Resumen

Este libro se divide en cinco partes. La primera parte aborda el problema del razonamiento con incertidumbre y los objetivos en las decisiones simples en un único instante. La segunda amplía la toma de decisiones a problemas secuenciales, en los que debemos tomar una secuencia de decisiones en respuesta a la información sobre los resultados de nuestras acciones a medida que avanzamos. La tercera aborda la incertidumbre del modelo, donde no empezamos con un modelo conocido y debemos aprender cómo actuar a través de la interacción con el entorno. La cuarta aborda la incertidumbre de estado, cuando la información perceptiva imperfecta nos impide conocer el estado completo del entorno. La última parte aborda los contextos de decisión en los que intervienen múltiples agentes.

## 1.6.1 Razonamiento probabilístico

![](./img/PR.png align=left width=30%)La toma de decisiones racional requiere razonar sobre nuestra incertidumbre y nuestros objetivos. Esta parte del libro comienza discutiendo cómo representar la incertidumbre como una distribución de probabilidad. Los problemas del mundo real requieren razonar sobre distribuciones de muchas variables. Discutiremos cómo construir estos modelos, cómo utilizarlos para hacer inferencias y cómo aprender sus parámetros y estructura a partir de los datos. A continuación, introducimos los fundamentos de la teoría de la utilidad y mostramos cómo ésta constituye la base para la toma de decisiones racionales en condiciones de incertidumbre a través del principio de *máxima utilidad esperada*. A continuación, analizamos cómo las nociones de la teoría de la utilidad pueden incorporarse a los modelos gráficos probabilísticos introducidos anteriormente en este capítulo para formar lo que se denominan *redes de decisión*.

## 1.6.2 Problemas secuenciales

![](./img/MDP.png align=right width=30%)Muchos problemas importantes requieren que tomemos una serie de decisiones. Se sigue aplicando el mismo principio de la máxima utilidad esperada, pero la toma de decisiones óptima en un contexto secuencial requiere razonar sobre futuras secuencias de acciones y observaciones. En esta parte del libro se analizarán los problemas de decisión secuencial en entornos estocásticos, donde los resultados de nuestras acciones son inciertos. Nos centraremos en una formulación general de los problemas de decisión secuencial bajo el supuesto de que el modelo es conocido y que el entorno es totalmente observable. Más adelante en el libro relajaremos ambos supuestos. Nuestra discusión comenzará con la introducción del *proceso de decisión de Markov* (MDP), el modelo matemático estándar para los problemas de decisión secuenciales. Discutiremos varios enfoques para encontrar soluciones exactas a este tipo de problemas. Dado que los problemas grandes a veces no permiten encontrar soluciones exactas de manera eficiente, discutiremos una colección de métodos de solución aproximada tanto offline como online, junto con un tipo de método que implica la búsqueda directa en el espacio de políticas de decisión parametrizadas. Por último, analizaremos los enfoques para validar que nuestras estrategias de decisión funcionarán como se espera cuando se apliquen en el mundo real.

## 1.6.3 Incertidumbre del modelo

![](./img/MU.png align=left width=30%)En nuestra discusión de los problemas de decisión secuencial hasta este punto, hemos asumido que los modelos de transición y recompensa son conocidos. En muchos problemas, sin embargo, la dinámica y las recompensas no se conocen con exactitud, y el agente debe aprender a actuar a través de la experiencia. Al observar los resultados de sus acciones en forma de transiciones de estado y recompensas, el agente debe elegir las acciones que maximicen su acumulación de recompensas a largo plazo. La resolución de este tipo de problemas en los que existe incertidumbre sobre el modelo es el tema del campo del aprendizaje por refuerzo y el tema central de esta parte del libro. Analizaremos varios retos a la hora de abordar la incertidumbre del modelo. En primer lugar, el agente debe equilibrar cuidadosamente la exploración del entorno con la explotación del conocimiento adquirido a través de la experiencia. En segundo lugar, las recompensas pueden recibirse mucho después de que se hayan tomado las decisiones importantes, por lo que el crédito de las recompensas posteriores debe asignarse a las decisiones anteriores. En tercer lugar, el agente debe generalizar a partir de una experiencia limitada. Revisaremos la teoría y algunos de los algoritmos clave para afrontar estos retos.

## 1.6.4 Incertidumbre de estado

![](./img/EI.jpg align=right width=30%)En esta parte, ampliamos la incertidumbre para incluir el estado. En lugar de observar el estado exactamente, recibimos observaciones que sólo tienen una relación probabilística con el estado. Estos problemas pueden modelarse como un *proceso de decisión de Markov parcialmente observable* (POMDP). Un enfoque común para resolver POMDPs implica inferir una distribución de creencias sobre el estado subyacente en el paso de tiempo actual y luego aplicar una política que mapea las creencias a las acciones. Esta parte comienza discutiendo cómo actualizar nuestra distribución de creencias, dada una secuencia pasada de observaciones y acciones. A continuación, se analizan diversos métodos exactos y aproximados para resolver los POMDP.

## 1.6.5 Sistemas multiagente

![](./img/MAS.png align=left width=30%)Hasta ahora, sólo ha habido un agente que tomaba decisiones en el entorno. En esta parte se amplían las cuatro partes anteriores a los agentes múltiples, discutiendo los retos que surgen de la incertidumbre de la interacción. Comenzamos hablando de juegos sencillos, en los que un grupo de agentes selecciona simultáneamente una acción. El resultado es una recompensa individual para cada agente basada en la acción conjunta combinada. El *juego de Markov* (MG) representa una generalización tanto de los juegos simples a múltiples estados como del MDP a múltiples agentes. En consecuencia, los agentes seleccionan acciones que pueden cambiar estocásticamente el estado de un entorno compartido. Los algoritmos para los MG dependen del aprendizaje por refuerzo debido a la incertidumbre sobre las políticas de los demás agentes. Un *juego de Markov parcialmente observable* (POMG) introduce la incertidumbre de estado, generalizando aún más los MG y los POMDP, ya que los agentes reciben ahora sólo observaciones locales ruidosas. El *proceso de decisión de Markov parcialmente observable descentralizado* (Dec-POMDP) centra el POMG en un equipo multiagente colaborativo en el que existe una recompensa compartida entre los agentes. Esta parte del libro presenta estas cuatro categorías de problemas y analiza los algoritmos exactos y aproximados que los resuelven.



<style class="fallback">body{visibility:hidden}</style><script>markdeepOptions={tocStyle:'medium'};</script>
<!-- Markdeep: --><script src="markdeep.min.js?" charset="utf-8"></script>

